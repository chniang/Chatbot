# ğŸ“š Importation des bibliothÃ¨ques
import nltk
from nltk.tokenize import PunktSentenceTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
import streamlit as st
import os

# ğŸ“ Configuration du chemin NLTK personnalisÃ©
NLTK_DATA_PATH = "C:\\Users\\Lenovo\\Desktop\\VSCODE_TRAINING\\myenv\\nltk_data"
if NLTK_DATA_PATH not in nltk.data.path:
    nltk.data.path.append(NLTK_DATA_PATH)

# ğŸ“¥ TÃ©lÃ©chargement des ressources nÃ©cessaires
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
except Exception:
    pass  # Ignore les erreurs de tÃ©lÃ©chargement

# ğŸ“„ Chargement du fichier texte
FILENAME = "Le livre Ã©lectronique du projet Gutenberg d'une autre Terre.txt"

lemmatizer = WordNetLemmatizer()
STOP_WORDS = stopwords.words('french')  # Utilisation du franÃ§ais

try:
    with open(FILENAME, 'r', encoding='utf-8') as f:
        data = f.read().replace('\n', ' ')
except FileNotFoundError:
    st.error(f"ERREUR : Le fichier '{FILENAME}' est introuvable. VÃ©rifiez le nom ou le chemin.")
    st.stop()

# âœ‚ï¸ Tokenisation en phrases avec PunktSentenceTokenizer
tokenizer = PunktSentenceTokenizer()
sentences = tokenizer.tokenize(data)

# ğŸ”§ Fonction de prÃ©traitement
def preprocess(sentence):
    """
    Nettoie et normalise une phrase : dÃ©coupage manuel, minuscule, suppression des mots vides/ponctuation, lemmatisation.
    """
    words = sentence.split()
    cleaned_words = []
    for word in words:
        word_lower = word.lower().strip(string.punctuation)
        if word_lower not in STOP_WORDS and word_lower.isalpha():
            cleaned_words.append(lemmatizer.lemmatize(word_lower))
    return cleaned_words

# ğŸ§¹ PrÃ©traitement du corpus
corpus = [preprocess(sentence) for sentence in sentences]

# ğŸ” Recherche par mots-clÃ©s
def get_most_relevant_sentence(query):
    query_keywords = preprocess(query)
    if not query_keywords:
        return "Veuillez poser une question contenant des mots-clÃ©s pertinents."

    best_match = ""
    max_overlap = 0

    for i, sentence_tokens in enumerate(corpus):
        overlap = len(set(query_keywords) & set(sentence_tokens))
        if overlap > max_overlap:
            max_overlap = overlap
            best_match = sentences[i]

    if max_overlap == 0:
        return "Aucune phrase du livre ne contient les mots-clÃ©s de votre question."

    return best_match

# ğŸ¤– Fonction principale du chatbot
def chatbot(question):
    return get_most_relevant_sentence(question)

# ğŸŒ Interface Streamlit
def main():
    st.set_page_config(page_title="Chatbot LittÃ©raire FranÃ§ais", layout="wide")
    st.title("ğŸ“š Chatbot du Livre 'Une autre Terre'")
    st.subheader("Posez une question sur le contenu du livre, et je vous rÃ©pondrai avec la phrase la plus pertinente.")
    
    question = st.text_input("Votre question :")
    
    if st.button("Obtenir une rÃ©ponse"):
        if not question:
            st.warning("Veuillez entrer une question.")
            return
        with st.spinner("Recherche en cours..."):
            response = chatbot(question)
            st.write("---")
            st.markdown(f"**RÃ©ponse du chatbot :** {response}")

# ğŸš€ Lancement de l'application
if __name__ == "__main__":
    main()
